{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, GPTNeoForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.cuda.device at 0x7fc20c38f2b0>,\n",
       " <torch.cuda.device at 0x7fc20c331de0>,\n",
       " <torch.cuda.device at 0x7fc20c331cf0>,\n",
       " <torch.cuda.device at 0x7fc20c331ba0>,\n",
       " <torch.cuda.device at 0x7fc20c330130>,\n",
       " <torch.cuda.device at 0x7fc20c330820>,\n",
       " <torch.cuda.device at 0x7fc20c3303a0>,\n",
       " <torch.cuda.device at 0x7fc0bbcba830>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:7' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "# model_name_or_path = \"princeton-nlp/Sheared-LLaMA-1.3B\"\n",
    "\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "# model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change cach dir for models\n",
    "CACHE_DIR = \"/data/pre-trained-models-cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmosbach/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_implementation = 'eager'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = AutoModelForCausalLM.from_pretrained(model_name_or_path, token=token, cache_dir=CACHE_DIR, attn_implementation=attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbde59e4d584af6809d9ccabd379f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load LLM2Vec transformed model\n",
    "# attn_implementation='flash_attention_2'\n",
    "# model_name_or_path = 'vaibhavad/llama-enc'\n",
    "model_name_or_path = 'vaibhavad/mistral-enc'\n",
    "lm = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.bfloat16, cache_dir=CACHE_DIR, attn_implementation=attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if attn_implementation == 'flash_attention_2':\n",
    "    tokenizer.padding_side  = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralConfig {\n",
       "  \"_name_or_path\": \"vaibhavad/mistral-enc\",\n",
       "  \"architectures\": [\n",
       "    \"MistralForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"model_type\": \"mistral\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.39.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n",
      "188\n",
      "torch.Size([1, 188])\n",
      "tensor([[    1, 25645,   349,   272,  1676,  1080,  1852,  9504,  2990,   297,\n",
      "          6082, 28725,   272,   261,  8016,  1080,  1852,  9504,  2990,   297,\n",
      "          3964,  4352, 28725,   304,   272,  1080,  1852,  9504,  2990,   297,\n",
      "           272, 14707,   302, 27798, 28723,  5196,   286,   297, 28705, 28740,\n",
      "         28784, 28781, 28750,   390,   550,  2457, 28733,  7308,   412, 28725,\n",
      "           442,   345, 22013,   302,  5480,   548, 28792, 28740, 28782, 28793,\n",
      "           378,   349,  5160,  1024,  7612,  8413, 28725, 28792, 28740, 28784,\n",
      "         28793,   272, 22212, 28733,   386,  6343, 12254,  1401,   690,   272,\n",
      "          2935,  2990,   302,   550,  2457, 28733,  7308,   412,   403,  4429,\n",
      "         20011, 28740, 28787, 28793,   415,  2990,   349,  1595,   893,   356,\n",
      "           272,  7633,   302, 25645, 28725,   690,  7365,   871,  1141,   477,\n",
      "           272,  1348,  5016,   390,   272,  2990, 28725, 28792, 28740, 28783,\n",
      "          3328, 28740, 28774, 28793,   304,   264,  1664,  1188,  7000, 27707,\n",
      "           282, 21029, 28725,   272,  7639,   302,   690,   349, 17417,   291,\n",
      "           365, 14640, 28723,   415,  2990,   349, 28705, 28740, 28774, 28784,\n",
      "          3535,   325, 28740, 28750, 28750,  5192, 28731,  9005,   302,   272,\n",
      "          4282,  5565, 28725, 14982,  8733, 28725,   304, 28705, 28750, 28782,\n",
      "         28783,  3535,   325, 28740, 28784, 28734,  5192, 28731,  6287,  7471,\n",
      "           302,   272, 26019,  5565, 28725, 27798,  3805, 28723]])\n",
      "['<s>', '▁Montreal', '▁is', '▁the', '▁second', '▁most', '▁pop', 'ulous', '▁city', '▁in', '▁Canada', ',', '▁the', '▁t', 'enth', '▁most', '▁pop', 'ulous', '▁city', '▁in', '▁North', '▁America', ',', '▁and', '▁the', '▁most', '▁pop', 'ulous', '▁city', '▁in', '▁the', '▁province', '▁of', '▁Quebec', '.', '▁Found', 'ed', '▁in', '▁', '1', '6', '4', '2', '▁as', '▁V', 'ille', '-', 'Mar', 'ie', ',', '▁or', '▁\"', 'City', '▁of', '▁Mary', '\",', '[', '1', '5', ']', '▁it', '▁is', '▁named', '▁after', '▁Mount', '▁Royal', ',', '[', '1', '6', ']', '▁the', '▁triple', '-', 'pe', 'aked', '▁hill', '▁around', '▁which', '▁the', '▁early', '▁city', '▁of', '▁V', 'ille', '-', 'Mar', 'ie', '▁was', '▁built', '.[', '1', '7', ']', '▁The', '▁city', '▁is', '▁cent', 'red', '▁on', '▁the', '▁Island', '▁of', '▁Montreal', ',', '▁which', '▁obtained', '▁its', '▁name', '▁from', '▁the', '▁same', '▁origin', '▁as', '▁the', '▁city', ',', '[', '1', '8', '][', '1', '9', ']', '▁and', '▁a', '▁few', '▁much', '▁smaller', '▁peripher', 'al', '▁islands', ',', '▁the', '▁largest', '▁of', '▁which', '▁is', '▁Î', 'le', '▁B', 'izard', '.', '▁The', '▁city', '▁is', '▁', '1', '9', '6', '▁km', '▁(', '1', '2', '2', '▁mi', ')', '▁east', '▁of', '▁the', '▁national', '▁capital', ',', '▁Ott', 'awa', ',', '▁and', '▁', '2', '5', '8', '▁km', '▁(', '1', '6', '0', '▁mi', ')', '▁south', 'west', '▁of', '▁the', '▁provincial', '▁capital', ',', '▁Quebec', '▁City', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Montreal is the second most populous city in Canada, the tenth most populous city in North America, and the most populous city in the province of Quebec. Founded in 1642 as Ville-Marie, or \"City of Mary\",[15] it is named after Mount Royal,[16] the triple-peaked hill around which the early city of Ville-Marie was built.[17] The city is centred on the Island of Montreal, which obtained its name from the same origin as the city,[18][19] and a few much smaller peripheral islands, the largest of which is Île Bizard. The city is 196 km (122 mi) east of the national capital, Ottawa, and 258 km (160 mi) southwest of the provincial capital, Quebec City.'\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "ids = tokenizer.encode(text, padding=\"do_not_pad\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "seq_len = len(tokens)\n",
    "input_ids = torch.tensor(ids).reshape(1, -1)\n",
    "\n",
    "print(seq_len)\n",
    "print(input_ids.shape)\n",
    "print(input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "         154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "         168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "         182, 183, 184, 185, 186, 187]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset = 0\n",
    "# offset = 4096 # TODO(mm): using any offset here results in CUDA errors. Try to figure out why.\n",
    "position_ids = torch.arange(start=offset, end=seq_len + offset).view(1, seq_len)\n",
    "position_ids.shape\n",
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_type = \"causal\"\n",
    "attention_type = \"bidirectional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable bidirectional attention\n",
    "attention_mask = None\n",
    "if attention_type == \"bidirectional\":\n",
    "    # construct attention mask (batch_size, 1, seq_len, seq_len)\n",
    "    attention_mask = torch.ones(size=(1, 1, seq_len, seq_len)).to(device)\n",
    "\n",
    "    if model_name_or_path in [\"princeton-nlp/Sheared-LLaMA-1.3B\", \"meta-llama/Llama-2-7b-hf\", \"meta-llama/Llama-2-7b-chat-hf\", 'vaibhavad/llama-enc']:\n",
    "        lm.model._update_causal_mask = lambda attention_mask, _: attention_mask\n",
    "\n",
    "    if model_name_or_path == \"EleutherAI/gpt-neo-1.3B\":\n",
    "        gpt_neo_max_length = 2048\n",
    "        bi_mask = torch.ones((1, 1, gpt_neo_max_length, gpt_neo_max_length), dtype=bool)\n",
    "\n",
    "        # overwrite causal mask at every layer\n",
    "        for lidx in range(len(lm.transformer.h)):\n",
    "            lm.transformer.h[lidx].attn.attention.bias = bi_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put inputs and model on GPU\n",
    "lm.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "position_ids = position_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 188])\n",
      "torch.Size([1, 188])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)\n",
    "print(position_ids.shape)\n",
    "# print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = input_ids\n",
    "output = lm.forward(input_ids=input_ids, position_ids=position_ids, labels=labels, attention_mask=attention_mask, output_attentions=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 6.2500000e-02 5.0354004e-03 ... 2.1057129e-03\n",
      "  8.7356567e-04 8.9111328e-03]\n",
      " [0.0000000e+00 0.0000000e+00 3.3007812e-01 ... 7.7819824e-04\n",
      "  7.5683594e-03 3.9367676e-03]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 1.1520386e-03\n",
      "  1.2874603e-04 1.3198853e-03]\n",
      " ...\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  4.5312500e-01 4.6081543e-03]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 3.6621094e-02]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# look at attention matrices\n",
    "# A = output.attentions[-1].squeeze()[-1]\n",
    "A = output.attentions[-1].squeeze()[-1].detach().cpu().float().numpy() \n",
    "print(np.triu(A, k=1)) # the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path = \"vaibhavad/mistral-enc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save attention matrices to disk\n",
    "data_path = f\"/data/attention_data/{model_name_or_path.split('/')[-1]}/{attention_type}\"\n",
    "\n",
    "# create dir\n",
    "Path(data_path).mkdir(parents=True, exist_ok=True)    \n",
    "    \n",
    "for layer in range(len(output.attentions)):\n",
    "    A = output.attentions[layer].squeeze().detach().cpu().float().numpy()\n",
    "    file_name = f\"A_layer{layer}.npy\"\n",
    "    with open(os.path.join(data_path, file_name), 'wb') as f:\n",
    "        np.save(f, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
