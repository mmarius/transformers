{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, GPTNeoForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "model_name_or_path = \"princeton-nlp/Sheared-LLaMA-1.3B\"\n",
    "\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "# model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change cach dir for models\n",
    "CACHE_DIR = \"/data/pre-trained-models-cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, token=token)\n",
    "attn_implementation = 'eager'\n",
    "lm = AutoModelForCausalLM.from_pretrained(model_name_or_path, token=token, cache_dir=CACHE_DIR, attn_implementation=attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load LLM2Vec transformed model\n",
    "# attn_implementation='flash_attention_2'\n",
    "# lm = AutoModelForCausalLM.from_pretrained('vaibhavad/mistral-enc', torch_dtype=torch.bfloat16, cache_dir=CACHE_DIR, attn_implementation=attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if attn_implementation == 'flash_attention_2':\n",
    "    tokenizer.padding_side  = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Montreal is the second most populous city in Canada, the tenth most populous city in North America, and the most populous city in the province of Quebec. Founded in 1642 as Ville-Marie, or \"City of Mary\",[15] it is named after Mount Royal,[16] the triple-peaked hill around which the early city of Ville-Marie was built.[17] The city is centred on the Island of Montreal, which obtained its name from the same origin as the city,[18][19] and a few much smaller peripheral islands, the largest of which is ÃŽle Bizard. The city is 196 km (122 mi) east of the national capital, Ottawa, and 258 km (160 mi) southwest of the provincial capital, Quebec City.'\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "ids = tokenizer.encode(text, padding=\"do_not_pad\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "seq_len = len(tokens)\n",
    "input_ids = torch.tensor(ids).reshape(1, -1)\n",
    "\n",
    "print(seq_len)\n",
    "print(input_ids.shape)\n",
    "print(input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "# offset = 4096 # TODO(mm): using any offset here results in CUDA errors. Try to figure out why.\n",
    "position_ids = torch.arange(start=offset, end=seq_len + offset).view(1, seq_len)\n",
    "position_ids.shape\n",
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_type = \"causal\"\n",
    "attention_type = \"bidirectional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable bidirectional attention\n",
    "attention_mask = None\n",
    "if attention_type == \"bidirectional\":\n",
    "    # construct attention mask (batch_size, 1, seq_len, seq_len)\n",
    "    attention_mask = torch.ones(size=(1, 1, seq_len, seq_len)).to(device)\n",
    "\n",
    "    if model_name_or_path in [\"princeton-nlp/Sheared-LLaMA-1.3B\", \"meta-llama/Llama-2-7b-hf\", \"meta-llama/Llama-2-7b-chat-hf\"]:\n",
    "        lm.model._update_causal_mask = lambda attention_mask, _: attention_mask\n",
    "\n",
    "    if model_name_or_path == \"EleutherAI/gpt-neo-1.3B\":\n",
    "        gpt_neo_max_length = 2048\n",
    "        bi_mask = torch.ones((1, 1, gpt_neo_max_length, gpt_neo_max_length), dtype=bool)\n",
    "\n",
    "        # overwrite causal mask at every layer\n",
    "        for lidx in range(len(lm.transformer.h)):\n",
    "            lm.transformer.h[lidx].attn.attention.bias = bi_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put inputs and model on GPU\n",
    "lm.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "position_ids = position_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_ids.shape)\n",
    "print(position_ids.shape)\n",
    "# print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = input_ids\n",
    "output = lm.forward(input_ids=input_ids, position_ids=position_ids, labels=labels, attention_mask=attention_mask, output_attentions=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at attention matrices\n",
    "# A = output.attentions[-1].squeeze()[-1]\n",
    "A = output.attentions[-1].squeeze()[-1].detach().cpu().float().numpy() \n",
    "print(np.triu(A, k=1)) # the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path = \"vaibhavad/mistral-enc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save attention matrices to disk\n",
    "# data_path = f\"/data/attention_data/{model_name_or_path.split('/')[-1]}/offset{offset}/{attention_type}\"\n",
    "data_path = f\"/data/attention_data/{model_name_or_path.split('/')[-1]}/{attention_type}\"\n",
    "\n",
    "# create dir\n",
    "Path(data_path).mkdir(parents=True, exist_ok=True)    \n",
    "    \n",
    "for layer in range(len(output.attentions)):\n",
    "    A = output.attentions[layer].squeeze().detach().cpu().float().numpy()\n",
    "    file_name = f\"A_layer{layer}.npy\"\n",
    "    with open(os.path.join(data_path, file_name), 'wb') as f:\n",
    "        np.save(f, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
